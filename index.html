<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link rel="icon" type="image/png" href="images/industrial-robot.ico">
  <!-- <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
  -->
  <script type="text/javascript" src="js/gtag.js"></script>
  <script type="text/javascript" src="js/hidebib.js"></script>
  <script type="text/javascript" src="js/imagehover.js"></script>
  
  <title>Roman Ibrahimov</title>
  <meta name="Roman's Homepage" http-equiv="Content-Type" content="Roman's Homepage">
  <link href="css/styles.css" rel="stylesheet" type="text/css">
  <link href="css/narrow_main.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>

<body>
<table style="width:100%;max-width:840px;" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<section class="home" id="home">
<table align="center" border="0" cellspacing="0" cellpadding="20">
  <p width="100%" align="center">
    <!-- <font size="7">Roman Ibrahimov</font><br> -->
    <pageheading>Roman Ibrahimov</pageheading><br>
    <b>email</b>:&nbsp
    <font id="email" style="display:inline;">
      roman[DOT]ibrahimov[AT]princeton[DOT]edu
    </font>
  </p>

  <tr>
    <td width="32%" valign="middle" align="center"><a><img src="images/RomanIbrahimov.jpg" width="100%" style="border-radius:200px"></a>
    <p align=center>
    <!-- <a href="data/CV_Youngjae.pdf" target="_blank">CV</a> | -->
    <a href="https://scholar.google.com/citations?hl=en&user=OUct8qMAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a> |
    <a href="https://www.youtube.com/@romanibrahimov811/videos" target="_blank">YouTube</a> 
    <!-- <a href="https://github.com/youngjae-min" target="_blank">Github</a> -->
    </p>
    </td>
    <td width="68%" valign="middle" align="justify">
    <p>
      I am a visiting Robotics scholar at  <a href="https://www.princeton.edu/" target="_blank">Princeton</a>'s 
      <a href="https://archfab.princeton.edu/" target="_blank">Embodied Computation Lab</a> and <a href="https://www.adelresearchgroup.org/about" target="_blank">ARG</a>. 
    </p> 
    <p>   
      I received my M.S. in Mechanical Engineering at <a href="https://me.berkeley.edu/" target="_blank">UC Berkeley</a>, working with <a href="https://me.berkeley.edu/people/mark-w-mueller/" target="_blank">Prof. Mark Mueller</a>  
      at <a href="https://hiperlab.berkeley.edu/" target="_blank">the High Performance Robotics Lab (HiPeRLab)</a>. 

      Previously, I studied Aeronautics and Astronautics at <a href="https://engineering.purdue.edu/AAE" target="_blank">Purdue University</a>, where I had the 
      privilege of working with <a href="https://engineering.purdue.edu/ME/People/ptProfile?id=57291" target="_blank">Prof. Shirley Dyke </a>and <a href="https://engineering.purdue.edu/ME/People/ptProfile?resource_id=92669" target="_blank">Prof. David Cappelleri</a> at the NASA-funded <a href="https://www.purdue.edu/rethi/" target="_blank">RETH Institute</a>.
    </p> 

      My research interests include robotics and control, with applications in hardware design, distributed 
      and collaborative multi-agent systems, human-robot interaction, and contact-rich manipulation with learning-based approaches. 
    <p> 

      <!-- <a href="link" target="_blank">name</a> -->
      <p> </p>

    </p>
<!--
    </p> -->
    </td>
  </tr>
</table>
</section>

<div id="navbar_outer">
<div id="navbar_tab">
<ul id="navbar">
  <li><a class="home_nav active" href="#home" onclick="make_active('home')">Home</a></li>
  <li><a class="research_nav" href="#research" onclick="make_active('research')">Research</a></li>
  <li><a class="teaching_nav" href="#teaching" onclick="make_active('teaching')">Teaching & Services</a></li>
  <!-- <li><a class="honor_nav" href="#honor" onclick="make_active('honor')">Honors</a></li> -->
  <!-- <li><a href="html/coursework.html">Related Coursework</a></li> -->
  <li><a href="data/cv_roman.pdf" target="_blank">CV</a></li>
</ul></div></div>
<br>

<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
    <li> newsline.</li>
    </ul>
  </td></tr>
</table>
-->

<section class="research" id="research">
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Research</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/layout_2.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/drone_series.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS"><a href="https://ieeexplore.ieee.org/abstract/document/11107817" class="_blank">Kalman Filter-Based Drift Detection and Mitigation of Visual-Inertial Odometry in UAVs</a></heading><br>
      <b>Roman Ibrahimov</b>, Teaya Yang, Mark Mueller<br>
      <em>American Control Conference (ACC)</em>, 2025
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://ieeexplore.ieee.org/abstract/document/11107817" target="_blank">Publisher</a> |
      <a href="javascript:toggleblock('acc_2025')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="acc_2025">
        We present a conceptual framework for an autonomous safety mechanism designed to enhance the reliability of Unmanned Aerial Vehicles (UAVs) 
        that use Visual-Inertial Odometry (VIO) for state estimation. As UAVs increasingly interact with the public, such safety mechanisms are crucial 
        to reducing the likelihood and severity of accidents. VIO drift, which occurs when accumulated estimation errors cause discrepancies between 
        the UAV’s perceived and actual position, poses a significant risk to safe operation. To address this challenge, we propose a Kalman 
        filter-based approach for detecting VIO drift events. Upon detection, the envisioned safety mechanism is designed to adjust state 
        estimation by integrating onboard gyroscope measurements and thrust commands for short durations, aiming to enhance stability and prevent 
        potential crashes before initiating a controlled landing. While this framework provides the foundation for a real-time safety mechanism, the 
        implementation and experiment focus on validating the drift detection component in an offline setting using real UAV flight data. The results
         demonstrate the effectiveness of the detection method in identifying VIO drift scenarios, highlighting its potential for future real-time 
         applications.


      </i></p>



<!-- ===+++++++++++++++++++++=== ICRA ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/drone_labelled.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/fruitcount.jpeg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">Towards Safe and Efficient Through-the-Canopy Autonomous Fruit Counting with UAVs</heading><br>
      Teaya Yang, <b>Roman Ibrahimov</b>, Mark Mueller<br>
      <em>International Conference on Robotics and Automation (ICRA)</em>, 2025
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://arxiv.org/abs/2409.18293" target="_blank">arXiv</a> |
      <a href="https://www.youtube.com/watch?v=8RJJ6P7-5gY" target="_blank">Video</a>  |
      <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  |
      <a href="javascript:toggleblock('icra_2025')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="icra_2025">
        We present an autonomous aerial system for safe and efficient through-the-canopy fruit counting. 
        Aerial robot applications in large-scale orchards face significant challenges due to the complexity 
        of fine-tuning flight paths based on orchard layouts, canopy density, and plant variability. 
        Through-the-canopy navigation is crucial for minimizing occlusion by leaves and branches but is more 
        challenging due to the complex and dense environment compared to traditional over-the-canopy flights. 
        Our system addresses these challenges by integrating: i) a high-fidelity simulation framework for optimizing
         flight trajectories, ii) a low-cost autonomy stack for canopy-level navigation and data collection, 
         and iii) a robust workflow for fruit detection and counting using RGB images. We validate our approach 
         through fruit counting with canopy-level aerial images and by demonstrating the autonomous navigation 
         capabilities of our experimental vehicle.


      </i></p>


<!-- +++++++++++++==================== -->






<!-- ===+++++++++++++++++++++=== ASME_24 ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/a_diag.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/Lunar_Igloo.jpg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">A Computational Framework for the Evaluation of Resilience in Deep Space Habitat Systems</heading><br>
      Amir Behjat, <b>Roman Ibrahimov</b>, Ali Lenjani, Aaron Barket, Kathleen Martinus, Amin Maghareh, Dawn Whitaker, Illias Bilionis, Shirley Dyke<br>
      <em>48th Design Automation Conference (DAC)</em>, 2022
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2022/V03AT03A036/1150405" target="_blank">Publisher</a> |
      <!-- <a href="https://www.youtube.com/watch?v=8RJJ6P7-5gY" target="_blank">Video</a>  |
      <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('ASME_24')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="ASME_24">
Resilience is a vital consideration for designing and operating a deep space habitat system. The numerous hazards that may affect a 
deep space habitat and its crew during its lifecycle need to be considered early in the design. Trade-off studies are the typical method
 used to assess the cost and value of different design choices. Here we develop a modular dynamic computational framework intended for 
 rapid simulation and evaluation of the resilience of different system configurations. The framework uses a system-level phenomeno-logical 
 Markov model of the habitat systems, enabling us to assess multiple habitat configurations and evaluate their performance in the presence 
 of several hazards and user-defined control policies. System fault detection and repairs are modeled. External disturbances, including 
 meteorites impact, temperature fluctuations, and dust, are modeled based on the lunar environment, envisioning a deep space habitat 
 design. We use a reflexive health management subsystem that prioritizes recovery actions based on the robotic agent’s availability to 
 close the loop. In addition to performance, a resilience metric is included to quantify the system’s resilience over the design lifecycle. 
 We illustrate the use of the framework for supporting early-stage design decisions of a habitat system. Our case study focuses on designing 
 the power generation system considering cost and energy efficiency.
      </i></p>


<!-- +++++++++++++==================== -->





<!-- ===+++++++++++++++++++++=== drone_light ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/final_hum_2.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/final_hum_1.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">DroneLight: Drone Draws in the Air using Long Exposure Light Painting and ML</heading><br>
      <b>Roman Ibrahimov</b>, Nikolay Zherdev, and Dzmitry Tsetserukou<br>
      <em>29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>, 2020
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://spectrum.ieee.org/video-friday-3d-printed-liquid-crystal-elastomer" target="_blank">Featured in IEEE Spectrum</a> |
      <a href="https://arxiv.org/pdf/2007.15171" target="_blank">arXiv</a> |
      <a href="https://www.youtube.com/watch?v=2azsfwnTOzc" target="_blank">Video</a>  |
      <!-- <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('drone_light')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="drone_light">
We propose a novel human-drone interaction
paradigm where a user directly interacts with a drone to
light-paint predefined patterns or letters through hand gestures. The user wears a glove which is equipped with an
IMU sensor to draw letters or patterns in the midair. The
developed ML algorithm detects the drawn pattern and the
drone light-paints each pattern in midair in the real time.
The proposed classification model correctly predicts all of the
input gestures. The DroneLight system can be applied in drone
shows, advertisements, distant communication through text or
pattern, rescue, and etc. To our knowledge, it would be the
worlds first human-centric robotic system that people can use
to send messages based on light-painting over distant locations
(drone-based instant messaging). Another unique application of
the system would be the development of vision-driven rescue
system that reads light-painting by person who is in distress
and triggers rescue alarm.
      </i></p>


<!-- +++++++++++++==================== -->








<!-- ===+++++++++++++++++++++=== drone_pick ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/VR_1.jpeg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/pic1.jpg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">DronePick: Object Picking and Delivery Teleoperation with the Drone Controlled by a Wearable Tactile Display </heading><br>
      <b>Roman Ibrahimov</b>, Evgeny Tsykunov, Vladimir Shirokun, Andrey Somov, Dzmitry Tsetserukou<br>
      <em>28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>, 2019
      </p>

      <div class="paper" id="cdc23_coils">
      <!-- <a href="https://spectrum.ieee.org/video-friday-3d-printed-liquid-crystal-elastomer" target="_blank">Featured in IEEE Spectrum</a> | -->
      <a href="https://arxiv.org/pdf/1908.02432" target="_blank">arXiv</a> |
      <a href="https://www.youtube.com/watch?v=6gAnmHqu1fc" target="_blank">Video</a>  |
      <!-- <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('drone_pick')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="drone_pick">
        We propose SlingDrone, a novel Mixed Reality interaction paradigm
        that utilizes a micro-quadrotor as both pointing controller and
        interactive robot with a slingshot motion type. The drone attempts
        to hover at a given position while the human pulls it in desired
        direction using a hand grip and a leash. Based on the displacement,
        a virtual trajectory is defined. To allow for intuitive and simple
        control, we use virtual reality (VR) technology to trace the path
        of the drone based on the displacement input. The user receives
        force feedback propagated through the leash. Force feedback from
        SlingDrone coupled with visualized trajectory in VR creates an
        intuitive and user friendly pointing device. When the drone is
        released, it follows the trajectory that was shown in VR. Onboard
        payload (e.g. magnetic gripper) can perform various scenarios for
        real interaction with the surroundings, e.g. manipulation or sensing.
        Unlike HTC Vive controller, SlingDrone does not require handheld
        devices, thus it can be used as a standalone pointing technology in
        VR.
      </i></p>


<!-- +++++++++++++==================== -->










<!-- ===+++++++++++++++++++++=== vrst ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/teaser_2.jpg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/teaser_1.jpg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">Slingdrone: Mixed reality system for pointing and interaction using a single drone </heading><br>
      Evgeny Tsykunov*, <b>Roman Ibrahimov*</b>, Derek Vasquez, Dzmitry Tsetserukou<br>
      <em>* - equal contribution</em><br>
      <em>25th ACM Symposium on Virtual Reality Software and Technology</em>, 2019
      </p>

      <div class="paper" id="cdc23_coils">
      <!-- <a href="https://spectrum.ieee.org/video-friday-3d-printed-liquid-crystal-elastomer" target="_blank">Featured in IEEE Spectrum</a> | -->
      <a href="https://arxiv.org/pdf/1911.04680" target="_blank">arXiv</a> |
      <a href="https://www.youtube.com/watch?v=bOhHF2Mqgjc" target="_blank">Video</a>  |
      <!-- <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('vrst')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="vrst">
We report on the teleoperation system DronePick which provides remote object picking and delivery by a human-controlled 
quadcopter. The main novelty of the proposed system is that the human user continuously  gets the visual and haptic feedback 
 for accurate teleoperation. DronePick consists of a quadcopter  equipped with a magnetic grabber, a tactile glove with 
 finger motion tracking sensor, hand tracking system, and the Virtual Reality (VR) application. The human operator 
 teleoperates the quadcopter by changing the position of the hand. The proposed vibrotactile patterns representing 
 the location of the remote object relative to the quadcopter are delivered to the glove. It helps the operator to 
 determine when the quadcopter is right above the object. When the ``pick'' command is sent by clasping the hand in 
 the glove, the quadcopter decreases its altitude and the magnetic grabber attaches the target object. The whole 
 scenario is in parallel simulated in VR. The air flow from the quadcopter and the relative positions of VR objects
  help the operator to determine the exact position of the delivered object to be picked. The experiments showed that 
  the vibrotactile patterns were recognized by the users at the high recognition rates: the  average 99\% recognition 
  rate and the average 2.36s recognition time. The real-life implementation of DronePick featuring object picking and 
  delivering to the human was developed and tested.
      </i></p>


<!-- +++++++++++++==================== -->

<!-- ===+++++++++++++++++++++=== s_touch ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/glove.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/front.jpg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">Swarmtouch: Guiding a swarm of micro-quadrotors with impedance control using a wearable tactile interface</heading><br>
      Evgeny Tsykunov, Ruslan Agishev, <b>Roman Ibrahimov</b>, Luiza Labazanova, Akerke Tleugazy, Dzmitry Tsetserukou<br>
      <em>IEEE Transactions on Haptics</em>, 2019
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://ieeexplore.ieee.org/iel7/4543165/8840984/08758191.pdf" target="_blank">Publisher</a> |
      <a href="https://www.youtube.com/watch?v=yvjUPv-Hu5I" target="_blank">Video</a>  |
      <!-- <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('s_touch')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="s_touch">
To achieve a smooth and safe guiding of a drone
formation by a human operator, we propose a novel interaction
strategy for a human–swarm communication, which combines
impedance control and vibrotactile feedback. The presented
approach takes into account the human hand velocity and
changes the formation shape and dynamics accordingly using
impedance interlinks simulated between quadrotors, which helps
to achieve a natural swarm behavior. Several tactile patterns
representing static and dynamic parameters of the swarm are
proposed. The user feels the state of the swarm at the fingertips
and receives valuable information to improve the controllability
of the complex formation. A user study revealed the patterns
with high recognition rates. A flight experiment demonstrated
the possibility to accurately navigate the formation in a cluttered
environment using only tactile feedback. Subjects stated that
tactile sensation allows guiding the drone formation through
obstacles and makes the human–swarm communication more
interactive. The proposed technology can potentially have a
strong impact on the human–swarm interaction, providing a
higher level of awareness during the swarm navigation.


      </i></p>


<!-- +++++++++++++==================== -->


<!-- ===+++++++++++++++++++++=== s_cloak ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/s_cloak_2.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/s_cloak_1.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">Swarmtouch: Guiding a swarm of micro-quadrotors with impedance control using a wearable tactile interface</heading><br>
      Evgeny Tsykunov, Ruslan Agishev, <b>Roman Ibrahimov</b>, Luiza Labazanova, Akerke Tleugazy, Dzmitry Tsetserukou<br>
      <em>SIGGRAPH Asia, Emerging Technologies</em>, 2019
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://arxiv.org/pdf/1911.09874" target="_blank">arXiv</a> |
      <a href="https://www.youtube.com/watch?v=uP-9lxI8g9E" target="_blank">Video</a>  |
      <!-- <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('s_cloak')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="s_cloak">
We propose a novel system SwarmCloak for landing of a fleet of four
flying robots on the human arms using light-sensitive landing pads
with vibrotactile feedback. We developed two types of wearable
tactile displays with vibromotors which are activated by the light
emitted from the LED array at the bottom of quadcopters. In a user
study, participants were asked to adjust the position of the arms
to land up to two drones, having only visual feedback, only tactile
feedback or visual-tactile feedback. The experiment revealed that
when the number of drones increases, tactile feedback plays a more
important role in accurate landing and operator’s convenience. An
important finding is that the best landing performance is achieved
with the combination of tactile and visual feedback. The proposed
technology could have a strong impact on the human-swarm interaction, providing a new level of intuitiveness and engagement into
the swarm deployment just right from the skin surface.

      </i></p>


<!-- +++++++++++++==================== -->



























<!-- ===+++++++++++++++++++++=== SE_24 ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/x_diag.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/Lunar_Habitat_with_Walker.jpeg" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">A computational framework for making early design decisions in deep space habitats</heading><br>
      Amir Behjat, Xiaoyu Liu, Oscar Forero, <b>Roman Ibrahimov</b>, Shirley Dyke, Ilias Bilionis, Julio Ramirez, Dawn Whitaker<br>
      <em>Advances in Engineering Software</em>, 2024
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://ntrs.nasa.gov/api/citations/20240008713/downloads/A%20Computational%20Framework.pdf" target="_blank">Publisher</a> |
      <!-- <a href="https://www.youtube.com/watch?v=8RJJ6P7-5gY" target="_blank">Video</a>  |
      <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('SE_24')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="SE_24">
       The dynamics of systems of systems often involve complex interactions among the individual systems, 
       making the implications of design choices challenging to predict. Design features in such systems may
        trigger unexpected behaviors or result in large variations in safety, performance or resilience. To 
        provide a means of simulating such systems for aiding in these decisions, we have developed a prototype 
        tool, the control-oriented dynamic computational modeling tool (CDCM).The CDCM provides rapid simulation
         capabilities to perform trade studies in systems of systems. The general class of systems of systems 
         that we aim to examine involve multiple hazards, damage, cascading consequences, repair and recovery. 
         We especially focus on systems-of-systems that incorporate a health management system (HMS) that can 
         monitor the state of the habitat and make decisions about actions to take. In this paper we describe 
         the features of the CDCM, the architecture we devised for simulation of systems-of-systems, the unique 
         functionalities of this tool, and we provide a demonstration of the capabilities by performing two 
         illustrative examples. We articulate the use of this tool for making early design decisions and demonstrate
          its use for trade studies that consider a model of a deep space habitat. We also share some experiences 
          and lessons that may be useful for others seeking to address similar problems.


      </i></p>


<!-- +++++++++++++==================== -->








<!-- ===+++++++++++++++++++++=== haptics ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/elec_diag.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/2_drones.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">SwarmCloak: Landing of Two Micro-Quadrotors on HumanHands Using Wearable Tactile Interface Driven by Light Intensity</heading><br>
      Evgeny Tsykunov, Ruslan Agishev, <b>Roman Ibrahimov</b>, Taha Moriyama, Luiza Labazanova, Hiroyuki Kajimoto, Dzmitry Tsetserukou<br>
      <em>IEEE Haptics Symposium (HAPTICS)</em>, 2020
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://arxiv.org/pdf/2001.11717" target="_blank">arXiv</a> |
      <!-- <a href="https://www.youtube.com/watch?v=8RJJ6P7-5gY" target="_blank">Video</a>  |
      <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('haptics')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="haptics">
  For the human operator, it is often easier and faster to catch a smallsize quadrotor right in the midair 
  instead of landing it on a surface.However, interaction strategies for such cases have not yet beenconsidered 
  properly, especially when more than one drone has tobe landed at the same time. In this paper, we 
  propose a novelinteraction strategy to land multiple robots on the human handsusing vibrotactile 
  feedback. We developed a wearable tactiledisplay that is activated by the intensity of light emitted 
  from anLED ring on the bottom of the quadcopter. We conductedexperiments, where participants were asked to 
  adjust the positionof the palm to land one or two vertically-descending drones withdifferent landing speeds, 
  by having only visual feedback, onlytactile feedback or visual-tactile feedback. We conducted statisticalanalysis 
  of the drone landing positions, landing pad and humanhead trajectories. Two-way ANOVA showed a statisticallysignificant
   difference between the feedback conditions.Experimental analysis proved that with an increasing number ofdrones, 
   tactile feedback plays a more important role in accuratehand positioning and operator’s convenience. The most 
   preciselanding of one and two drones was achieved with the combinationof tactile and visual feedback.


      </i></p>


<!-- +++++++++++++==================== -->








<!-- ===+++++++++++++++++++++=== aerovr ============================== -->

<tr onmouseout="hide_new(this)" onmouseover="show_new(this)">
    <td width="33%" valign="center" align="center">
    <div class ="one">
      <div class = "two" style="opacity:0">
        <img src="images/papers/vr_glove.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
      </div>
      <img class="default_image" src="images/papers/vr.png" alt="sym" width="90%" style="padding:2px;border-radius:15px">
    </div>
    
    </td>
    <td width="67%" valign="top">
      <p>
      <heading id="CDC23_COILS">AeroVR: Virtual Reality-based Teleoperation with Tactile Feedback
for Aerial Manipulation</heading><br>
      Grigoriy Yashin, Daria Trinitatova, Ruslan Agishev, <b>Roman Ibrahimov</b>, and Dzmitry Tsetserukou<br>
      <em>19th International Conference on Advanced Robotics (ICAR)</em>, 2019
      </p>

      <div class="paper" id="cdc23_coils">
      <a href="https://arxiv.org/pdf/1910.11604" target="_blank">arXiv</a> |
      <a href="https://www.youtube.com/watch?v=XKKIfeOnHyk" target="_blank">Video</a>  |
      <!-- <a href="https://teaya-yang.github.io/papers/ICRA2025_poster.pdf" target="_blank">Poster</a>  | -->
      <a href="javascript:toggleblock('aerovr')">Abstract</a><!--
      <a shape="rect" href="javascript:togglebib('cdc23_coils')" class="togglebib">bibtex</a>-->

      <p align="justify"> <i id="aerovr">
Drone application for aerial manipulation is tested
in such areas as industrial maintenance, supporting the rescuers
in emergencies, and e-commerce. Most of such applications
require teleoperation. The operator receives visual feedback
from the camera installed on a robot arm or drone. As aerial
manipulation requires delicate and precise motion of robot
arm, the camera data delay, narrow field of view, and blurred
image caused by drone dynamics can lead the UAV to crash.
The paper focuses on the development of a novel teleoperation
system for aerial manipulation using Virtual Reality (VR). The
controlled system consists of UAV with a 4-DoF robotic arm
and embedded sensors. VR application presents the digital twin
of drone and remote environment to the user through a headmounted display (HMD). The operator controls the position
of the robotic arm and gripper with VR trackers worn on
the arm and tracking glove with vibrotactile feedback. Control
data is translated directly from VR to the real robot in realtime. The experimental results showed a stable and robust
teleoperation mediated by the VR scene. The proposed system
can considerably improve the quality of aerial manipulations.

      </i></p>


<!-- +++++++++++++==================== -->


















</table>
<!-- </section>

<br>
<section class="honor" id="honor">
<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;Honors and Awards</sectionheading>
    <ul>
    <li> KAIST Global Leadership Award (2020)</li>
    <li> KAIST Presidential Fellowship (2014-2020)</li>
    <li> GE Foundation Scholar-Leaders Program (2015-2019)</li>
    <li> KFAS Undergraduate Scholarship (2015-2019) </li>
    <li> US Army Commendation Medal (2017)</li>
    </ul>
  </td></tr>
</table>
</section> -->


<section class="teaching" id="teaching">
<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td>
    <!-- <sectionheading>&nbsp;&nbsp;Teaching and Services</sectionheading>
    <ul>
    <li> KAIST Global Leadership Award (2020)</li>
    <li> KAIST Presidential Fellowship (2014-2020)</li>
    <li> GE Foundation Scholar-Leaders Program (2015-2019)</li>
    <li> KFAS Undergraduate Scholarship (2015-2019) </li>
    <li> US Army Commendation Medal (2017)</li>
    </ul> -->
    <section style="width:100%; margin:0 auto; padding:16px;">
            <h2>Teaching &amp; Services</h2>
        
            <!-- Teaching subsection -->
            <div style="margin-bottom:24px;">
                <h3>Teaching</h3>
                <ul style="list-style-position: inside; padding-left: 20px; margin-top: 8px;">
                  <li><strong>Graduate Student Instructor</strong> - ENGIN 7: Computer Programming for Scientists and Engineers
 (Spring 2025, ~20 hrs/week)</li>
                  <li><strong>Teaching Assistant</strong> - CNIT 155 Introduction to Software Development Concepts (in Python) (Spring 2021, Fall 2021, Spring 2022 ~20 hrs/week)</li>
                  <li><strong>Teacher</strong> - Cambridge K12 IGCSE Information and Communication Technology (Fall 2020, ~40 hrs/week)</li>
                  <!-- <li><strong>Teaching Assistant</strong> - PHYS2213: Electromagnetism (Spring 2020, ~3 hrs/week)</li>
                  <li><strong>Teaching Assistant</strong> - PHYS2214: Oscillations, Waves, and Quantum Physics (Fall 2020, Spring 2021, ~3 hrs/week)</li>
                  <li><strong>Course Consultant</strong> - CS1112: Intro to MATLAB (Spring 2020, ~5 hrs/week)</li>
                  <li><strong>Math Tutor</strong> at Ithaca High School - Precalculus, Geometry (Fall 2019, ~2 hrs/week)</li> -->
                </ul>
              </div>
              
        
            <!-- Mentoring subsection -->
            <div style="margin-bottom:24px;">
              <h3>Mentoring</h3>
              <ul style="list-style-position: inside; padding-left: 20px; margin-top: 8px;">
                <!-- <li>Dylan Lee – MS student, UC Berkeley (2023–2025)</li> -->
                <li>Jannik Matthias Heinen, – MEng student, UC Berkeley (2024–2025)</li>
                <li>Milan Rosic – MEng student, UC Berkeley (2024–2025)</li>
                <li>Yuxuan Peng – MEng student, UC Berkeley (2024–2025)</li>
                <li>Ruonan Yang – MEng student, UC Berkeley (2024–2025)</li>
                <li>Anderson Xu – Undergraduate student, Purdue University (2022–2023)</li>
                <li>Benjamin Krugman – Undergraduate student, Purdue University (2022-2023)</li>
                <!-- <li>Wentao Zhang – Undergraduate student, ShanghaiTech University (2023)</li> -->
              </ul>
            </div>
        
            <!-- Academic Service subsection -->
            <div style="margin-bottom:24px;">
            <h3>Academic Service</h3>
            <ul style="list-style-position: inside; padding-left: 20px; margin-top: 8px;">
              <li> <strong>Reviewer for Journal and Conferences: </strong>IEEE Robotics and Automation Letters (RA-L) 2024, RA-L 2023, RA-L 2022, IEEE International Conference on Robotics and Automation (ICRA) 2024, ICRA 2023, ICRA 2021, ICRA 2020; ACM Conference on Human Factors in Computing Systems (CHI) 2020; Virtual Reality & Intelligent Hardware Journal 2020;
              </li>
              
            </ul>
            </div>

            <!-- Outreach subsection
            <div style="margin-bottom:24px;">
              <h3>Outreach</h3>
              <ul style="list-style-position: inside; padding-left: 20px; margin-top: 8px;">
                <li>Organized lab visit and activities for high school students through the Berkeley <a href="https://met.berkeley.edu/about-met/met-ia/">MET</a> summer program (2024, 2025)</li>
                <li>STEM outreach talk and showcase at <a href="https://ebinternacional.org/">EBI</a> Middle School (2023)</li>
                <li>Mentored a summer undergraduate researcher through the <a href="https://eecs.berkeley.edu/resources/undergrads/undergraduate-research/superb/">SUPERB</a> program at Berkeley (2023)</li>
              </ul>
              </div> -->
        </section>





  </td></tr>
</table>
</section>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Template modified from <a href="http://www.cs.berkeley.edu/~barron/">this</a> and <a href="https://youngjae-min.github.io/index.html#home">this</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('acc_2025');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icra_2025');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('SE_24');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ASME_24');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('drone_light');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('drone_pick');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('vrst');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('s_touch');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('s_cloak');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('haptics');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('aerovr');
</script>










<script xml:space="preserve" language="JavaScript">
hideblock('icml23_sogd_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icra23_dsk3dom_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cdc22_orfit_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icra21_kdogm_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ijcas_gpssm_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('access_admm_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('gnc20_gps_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('isit19_nn_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jksnt_slowness_abs');
</script>
<!--<script xml:space="preserve" language="JavaScript">
hideblock('ius14_friction_abs');
</script>-->
<script src="js/navscroll.js"></script>
</body>

</html>
